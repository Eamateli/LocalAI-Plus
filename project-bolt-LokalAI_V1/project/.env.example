# LocalAI+ Configuration

# Server Settings
HOST=0.0.0.0
PORT=8000
DEBUG=false

# Security
SECRET_KEY=your-secret-key-change-this-in-production
API_KEYS=["localai-plus-default-key","your-custom-api-key"]
REQUIRE_AUTH=true
ALLOWED_ORIGINS=["http://localhost:3000","https://yourdomain.com"]
ALLOWED_HOSTS=["*"]

# Database
DATABASE_URL=postgresql://localai:localai123@localhost:5432/localai

# Redis
REDIS_URL=redis://localhost:6379

# Model Services  
OLLAMA_BASE_URL=http://localhost:11434
VLLM_BASE_URL=http://localhost:8080

# Vector Database
QDRANT_URL=http://localhost:6333
CHROMA_PERSIST_DIR=./chroma_db

# Model Settings
DEFAULT_MODEL=llama2:7b
DEFAULT_EMBEDDING_MODEL=all-MiniLM-L6-v2
MAX_CONTEXT_LENGTH=4096
MAX_CONCURRENT_REQUESTS=10

# Code Execution
ENABLE_CODE_EXECUTION=true
CODE_EXECUTION_TIMEOUT=30

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=3600

# RAG Settings
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_TOP_K=5

# Frontend
VITE_API_URL=http://localhost:8000
VITE_API_KEY=localai-plus-default-key